## 爬取当当网 Top 500 本书
* 
## 


```python
import requests
import re
import json

#拿源码,拿到了就用response.text 返回。
def request_dandan(url):
    try:
        response = requests.get(url)
        if response.status_code == 200:
            return response.text
    except requests.RequestException as e:
        print(e)
        return None

#从返回的text中查找要的数据，写成列表，然后提取列表到字典。
def parse_result(html):
    pattern = re.compile(
        '<li.*?list_num.*?(\d+)\.</div>.*?<img src="(.*?)".*?class="name".*?title="(.*?)">.*?class="star">.*?class="tuijian">(.*?)</span>.*?class="publisher_info">.*?target="_blank">(.*?)</a>.*?class="biaosheng">.*?<span>(.*?)</span></div>.*?<p><span class="price_n">(.*?)</span>.*?</li>', re.S)
    items = re.findall(pattern, html)

    for item in items:
        #yield可以看作是 return ，可迭代的
        yield {
            'range': item[0],
            'image': item[1],
            'title': item[2],
            'recommend': item[3],
            'author': item[4],
            'times': item[5],
            'price': item[6]
        }

#
def write_item_to_file(item):
    print('开始写入数据 ====> ' + str(item))
    with open('book.txt', 'a', encoding='UTF-8') as f:
        f.write(json.dumps(item, ensure_ascii=False) + '\n')

#实现自动翻页
def main(page):
    url = 'http://bang.dangdang.com/books/fivestars/01.00.00.00.00.00-recent30-0-0-1-' + str(page)
    html = request_dandan(url)
    items = parse_result(html)  # 解析过滤我们想要的信息
    for item in items:
        write_item_to_file(item)


if __name__ == "__main__":
    for i in range(1, 26):
        main(i)
```
***
## 当当网top250榜单数据
```python
import requests
from bs4 import BeautifulSoup
import openpyxl
from openpyxl import Workbook


#链接网页，返回数据
def request(page):
    url = 'https://movie.douban.com/top250?start='+ str(page * 25)+ '&filter='
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) '
                      'Chrome/88.0.4324.146 Safari/537.36',
    }
    try:
        re = requests.get(url = url, headers = headers)
        #放进汤里
        soup = BeautifulSoup(re.text, 'lxml')
        return soup
    except:
        print('链接失败')

        
#从汤里把需要的数据提取出来，返回列表
def dataList(soup):
    items = soup.find(class_ = "grid_view").find_all('li')
    datalists = []
    for item in items:
        name = item.find('span', class_ = "title").string
        picture = item.find('a').find('img').get('src')
        rank = item.find('em', class_ = "").string
        grade = item.find('span', class_="rating_num").string
        author = item.find('p', class_="").text
        if item.find('span', class_ = "inq") is not None:
            synopsis = item.find('span', class_ = "inq").string
        else:
            synopsis = "这逼没有简介的"
        datalist = [name, picture, rank, grade, author.strip(), synopsis]
        datalists.append(datalist)
    return datalists
        
    
def main():
    #开一份excel
    book = Workbook()
    sheet = book.active
    #遍历1-10页
    for i in range(0,10):
        page = i
        soup = request(page)
        datas = dataList(soup)
        #把数据写入到excel
        for data in datas:
            sheet.append(data)
    #保存excel
    book.save(u'豆瓣top250.xlsx')

    
if __name__ == "__main__":
    main()
```
